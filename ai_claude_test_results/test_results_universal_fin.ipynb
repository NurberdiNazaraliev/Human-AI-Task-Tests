{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f85561a-c831-4ecb-b9fb-88ae38a76b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurbe\\AppData\\Local\\Temp\\ipykernel_27044\\3486449697.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.results_df = pd.concat([self.results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tested .\\task0.py ‚Üí Accuracy 100.0%, Time 0.0001s, Memory 0.07KB\n",
      "‚úÖ Tested .\\task0ai.py ‚Üí Accuracy 100.0%, Time 0.0000s, Memory 0.08KB\n",
      "\n",
      "üìä SUMMARY RESULTS\n",
      "         File Function  Accuracy    Time_s  Memory_KB Complexity  \\\n",
      "0    task0.py    task0       1.0  0.000054   0.070312          3   \n",
      "1  task0ai.py  task0ai       1.0  0.000045   0.078125          3   \n",
      "\n",
      "   Maintainability  Pylint_Score  \n",
      "0        72.431958          10.0  \n",
      "1        74.285292          10.0  \n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import tracemalloc\n",
    "import pandas as pd\n",
    "from radon.complexity import cc_visit\n",
    "from radon.metrics import mi_visit\n",
    "\n",
    "class FunctionTester:\n",
    "    def __init__(self, test_cases):\n",
    "        self.test_cases = test_cases\n",
    "        self.results_df = pd.DataFrame(columns=[\n",
    "            \"File\", \"Function\", \"Accuracy\", \"Time_s\", \"Memory_KB\", \"Complexity\", \"Maintainability\", \"Pylint_Score\"\n",
    "        ])\n",
    "    \n",
    "    def load_function(self, file_path):\n",
    "        \"\"\"Load the main function from a Python file (auto-detect function name).\"\"\"\n",
    "        spec = importlib.util.spec_from_file_location(\"module\", file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        # Find candidate functions\n",
    "        candidates = [attr for attr in dir(module) if callable(getattr(module, attr)) and not attr.startswith(\"_\")]\n",
    "        for func_name in candidates:\n",
    "            if any(keyword in func_name.lower() for keyword in ['task','func','main','solution']):\n",
    "                return getattr(module, func_name), func_name\n",
    "        if candidates:\n",
    "            return getattr(module, candidates[0]), candidates[0]\n",
    "        raise AttributeError(f\"No callable function found in {file_path}\")\n",
    "    \n",
    "    def run_test(self, func, test_cases, repeat: int = 3):\n",
    "        \"\"\"Run all test cases, measure accuracy, peak memory, and time.\"\"\"\n",
    "        correct = 0\n",
    "        peak_memory_kb = 0\n",
    "        start_total = time.perf_counter()\n",
    "        \n",
    "        for idx, test_case in enumerate(test_cases):\n",
    "            try:\n",
    "                args, expected = test_case if isinstance(test_case, tuple) else (test_case, test_case[1])\n",
    "                if not isinstance(args, tuple):\n",
    "                    args = (args,)\n",
    "                \n",
    "                # Repeat function execution for smoother timing\n",
    "                mem_peak_case = 0\n",
    "                result = None\n",
    "                for _ in range(repeat):\n",
    "                    tracemalloc.start()\n",
    "                    start = time.perf_counter()\n",
    "                    result = func(*args)\n",
    "                    end = time.perf_counter()\n",
    "                    current, peak = tracemalloc.get_traced_memory()\n",
    "                    tracemalloc.stop()\n",
    "                    mem_peak_case = max(mem_peak_case, peak / 1024.0)\n",
    "                \n",
    "                peak_memory_kb = max(peak_memory_kb, mem_peak_case)\n",
    "                if self.compare_results(result, expected):\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    print(f\"‚ùå Input {args} ‚Üí Expected {expected}, Got {result}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error with input {args}: {e}\")\n",
    "        \n",
    "        end_total = time.perf_counter()\n",
    "        execution_time = end_total - start_total\n",
    "        accuracy = correct / len(test_cases)\n",
    "        return accuracy, execution_time, peak_memory_kb\n",
    "    \n",
    "    def compare_results(self, result, expected):\n",
    "        if result == expected:\n",
    "            return True\n",
    "        if isinstance(result, (list,set)) and isinstance(expected, (list,set)):\n",
    "            return sorted(result) == sorted(expected)\n",
    "        if isinstance(result, float) and isinstance(expected, float):\n",
    "            return abs(result-expected)<1e-9\n",
    "        return False\n",
    "    \n",
    "    def analyze_code_quality(self, file_path):\n",
    "        \"\"\"Complexity, maintainability (radon), pylint.\"\"\"\n",
    "        try:\n",
    "            with open(file_path,'r',encoding='utf-8') as f:\n",
    "                code = f.read()\n",
    "            complexity = sum(block.complexity for block in cc_visit(code))\n",
    "            maintainability = mi_visit(code, True)\n",
    "            pylint_score = self.get_pylint_score(file_path)\n",
    "            return complexity, maintainability, pylint_score\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Code analysis failed for {file_path}: {e}\")\n",
    "            return 0,0,0\n",
    "    \n",
    "    def get_pylint_score(self, file_path):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"pylint\", file_path, \"--score=y\", \"--disable=R,C,W\"],\n",
    "                capture_output=True, text=True, timeout=30\n",
    "            )\n",
    "            for line in result.stdout.splitlines():\n",
    "                if \"rated at\" in line:\n",
    "                    return float(line.split(\"rated at\")[1].split(\"/\")[0].strip())\n",
    "        except Exception:\n",
    "            pass\n",
    "        return 0.0\n",
    "    \n",
    "    def record_results(self, file_path, function_name, accuracy, time_s, memory_kb, complexity, maintainability, pylint_score):\n",
    "        self.results_df = pd.concat([self.results_df, pd.DataFrame([{\n",
    "            \"File\": os.path.basename(file_path),\n",
    "            \"Function\": function_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Time_s\": time_s,\n",
    "            \"Memory_KB\": memory_kb,\n",
    "            \"Complexity\": complexity,\n",
    "            \"Maintainability\": maintainability,\n",
    "            \"Pylint_Score\": pylint_score\n",
    "        }])], ignore_index=True)\n",
    "    \n",
    "    def test_file(self, file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ùå {file_path} not found\")\n",
    "            return\n",
    "        func, func_name = self.load_function(file_path)\n",
    "        test_cases = self.test_cases.get(os.path.basename(file_path).replace(\".py\",\"\"), [])\n",
    "        accuracy, time_s, memory_kb = self.run_test(func, test_cases)\n",
    "        complexity, maintainability, pylint_score = self.analyze_code_quality(file_path)\n",
    "        self.record_results(file_path, func_name, accuracy, time_s, memory_kb, complexity, maintainability, pylint_score)\n",
    "        print(f\"‚úÖ Tested {file_path} ‚Üí Accuracy {accuracy:.1%}, Time {time_s:.4f}s, Memory {memory_kb:.2f}KB\")\n",
    "    \n",
    "    def run_all(self, participant_dir=\".\"):\n",
    "        for filename in self.test_cases.keys():\n",
    "            file_path = os.path.join(participant_dir, f\"{filename}.py\")\n",
    "            self.test_file(file_path)\n",
    "        print(\"\\nüìä SUMMARY RESULTS\")\n",
    "        print(self.results_df)\n",
    "        return self.results_df\n",
    "\n",
    "# Example main\n",
    "def main():\n",
    "    TEST_CASES = {\n",
    "        \"task0\": [\n",
    "            ([1,2,3,4,5], 2),\n",
    "            ([], 0),\n",
    "            ([2,4,6,8], 4),\n",
    "            ([1,3,5], 0),\n",
    "        ],\n",
    "        \"task0ai\": [\n",
    "            ([1,2,3,4,5],[1,3,5]),\n",
    "            ([], []),\n",
    "            ([2,4,6,8], []),\n",
    "            ([1,3,5],[1,3,5]),\n",
    "        ],\n",
    "    }\n",
    "    tester = FunctionTester(TEST_CASES)\n",
    "    results_df = tester.run_all()\n",
    "    results_df.to_csv(\"task0_results.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b36b28-5bf4-4226-a58f-25b76751d44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurbe\\AppData\\Local\\Temp\\ipykernel_27044\\1220887177.py:124: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.results_df = pd.concat([self.results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tested .\\task1.py ‚Üí Accuracy 100.0%, Time 0.0001s, Memory 0.11KB\n",
      "‚úÖ Tested .\\task1ai.py ‚Üí Accuracy 100.0%, Time 0.0001s, Memory 0.11KB\n",
      "\n",
      "üìä SUMMARY RESULTS\n",
      "         File Function  Accuracy    Time_s  Memory_KB Complexity  \\\n",
      "0    task1.py    task1       1.0  0.000106   0.109375          4   \n",
      "1  task1ai.py  task1ai       1.0  0.000108   0.109375          5   \n",
      "\n",
      "   Maintainability  Pylint_Score  \n",
      "0        72.464997          10.0  \n",
      "1        70.913972          10.0  \n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import tracemalloc\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from radon.complexity import cc_visit\n",
    "from radon.metrics import mi_visit\n",
    "\n",
    "class Task1Tester:\n",
    "    def __init__(self, test_cases):\n",
    "        self.test_cases = test_cases\n",
    "        self.results_df = pd.DataFrame(columns=[\n",
    "            \"File\", \"Function\", \"Accuracy\", \"Time_s\", \"Memory_KB\", \"Complexity\", \"Maintainability\", \"Pylint_Score\"\n",
    "        ])\n",
    "    \n",
    "    def load_function(self, file_path):\n",
    "        \"\"\"Load the main function from a Python file (auto-detect function name).\"\"\"\n",
    "        spec = importlib.util.spec_from_file_location(\"module\", file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        # Find candidate functions\n",
    "        candidates = [attr for attr in dir(module) if callable(getattr(module, attr)) and not attr.startswith(\"_\")]\n",
    "        for func_name in candidates:\n",
    "            if any(keyword in func_name.lower() for keyword in ['task','func','main','solution']):\n",
    "                return getattr(module, func_name), func_name\n",
    "        if candidates:\n",
    "            return getattr(module, candidates[0]), candidates[0]\n",
    "        raise AttributeError(f\"No callable function found in {file_path}\")\n",
    "    \n",
    "    def run_test(self, func, test_cases, repeat: int = 3):\n",
    "        \"\"\"Run all test cases, measure accuracy, peak memory, and time.\"\"\"\n",
    "        correct = 0\n",
    "        peak_memory_kb = 0\n",
    "        start_total = time.perf_counter()\n",
    "        \n",
    "        for idx, test_case in enumerate(test_cases):\n",
    "            try:\n",
    "                (input_args, valid_answers) = test_case\n",
    "                nums, target = input_args\n",
    "                \n",
    "                # Repeat function execution for smoother timing\n",
    "                mem_peak_case = 0\n",
    "                result = None\n",
    "                for _ in range(repeat):\n",
    "                    tracemalloc.start()\n",
    "                    start = time.perf_counter()\n",
    "                    result = func(nums, target)\n",
    "                    end = time.perf_counter()\n",
    "                    current, peak = tracemalloc.get_traced_memory()\n",
    "                    tracemalloc.stop()\n",
    "                    mem_peak_case = max(mem_peak_case, peak / 1024.0)\n",
    "                \n",
    "                peak_memory_kb = max(peak_memory_kb, mem_peak_case)\n",
    "                \n",
    "                # Handle case where function returns None instead of empty list\n",
    "                if result is None and valid_answers == [[]]:\n",
    "                    result = []\n",
    "                \n",
    "                if self.is_valid_answer(result, nums, target, valid_answers):\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    print(f\"‚ùå Input nums={nums}, target={target} ‚Üí Expected one of {valid_answers}, Got {result}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error with input nums={nums}, target={target}: {e}\")\n",
    "        \n",
    "        end_total = time.perf_counter()\n",
    "        execution_time = end_total - start_total\n",
    "        accuracy = correct / len(test_cases)\n",
    "        return accuracy, execution_time, peak_memory_kb\n",
    "    \n",
    "    def is_valid_answer(self, result, nums, target, valid_answers):\n",
    "        \"\"\"Check if the result is any of the valid answers.\"\"\"\n",
    "        if result == [] and valid_answers == [[]]:\n",
    "            return True\n",
    "        \n",
    "        if not isinstance(result, list) or len(result) != 2:\n",
    "            return False\n",
    "        \n",
    "        i, j = result\n",
    "        \n",
    "        # Check indices are valid\n",
    "        if not (0 <= i < len(nums) and 0 <= j < len(nums) and i != j):\n",
    "            return False\n",
    "        \n",
    "        # Check if this matches any valid answer (order doesn't matter for indices)\n",
    "        for valid in valid_answers:\n",
    "            if valid == []:\n",
    "                continue\n",
    "            if set(result) == set(valid):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def analyze_code_quality(self, file_path):\n",
    "        \"\"\"Complexity, maintainability (radon), pylint.\"\"\"\n",
    "        try:\n",
    "            with open(file_path,'r',encoding='utf-8') as f:\n",
    "                code = f.read()\n",
    "            complexity = sum(block.complexity for block in cc_visit(code))\n",
    "            maintainability = mi_visit(code, True)\n",
    "            pylint_score = self.get_pylint_score(file_path)\n",
    "            return complexity, maintainability, pylint_score\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Code analysis failed for {file_path}: {e}\")\n",
    "            return 0,0,0\n",
    "    \n",
    "    def get_pylint_score(self, file_path):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"pylint\", file_path, \"--score=y\", \"--disable=R,C,W\"],\n",
    "                capture_output=True, text=True, timeout=30\n",
    "            )\n",
    "            for line in result.stdout.splitlines():\n",
    "                if \"rated at\" in line:\n",
    "                    return float(line.split(\"rated at\")[1].split(\"/\")[0].strip())\n",
    "        except Exception:\n",
    "            pass\n",
    "        return 0.0\n",
    "    \n",
    "    def record_results(self, file_path, function_name, accuracy, time_s, memory_kb, complexity, maintainability, pylint_score):\n",
    "        self.results_df = pd.concat([self.results_df, pd.DataFrame([{\n",
    "            \"File\": os.path.basename(file_path),\n",
    "            \"Function\": function_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Time_s\": time_s,\n",
    "            \"Memory_KB\": memory_kb,\n",
    "            \"Complexity\": complexity,\n",
    "            \"Maintainability\": maintainability,\n",
    "            \"Pylint_Score\": pylint_score\n",
    "        }])], ignore_index=True)\n",
    "    \n",
    "    def test_file(self, file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ùå {file_path} not found\")\n",
    "            return\n",
    "        func, func_name = self.load_function(file_path)\n",
    "        test_cases = self.test_cases.get(os.path.basename(file_path).replace(\".py\",\"\"), [])\n",
    "        accuracy, time_s, memory_kb = self.run_test(func, test_cases)\n",
    "        complexity, maintainability, pylint_score = self.analyze_code_quality(file_path)\n",
    "        self.record_results(file_path, func_name, accuracy, time_s, memory_kb, complexity, maintainability, pylint_score)\n",
    "        print(f\"‚úÖ Tested {file_path} ‚Üí Accuracy {accuracy:.1%}, Time {time_s:.4f}s, Memory {memory_kb:.2f}KB\")\n",
    "    \n",
    "    def run_all(self, participant_dir=\".\"):\n",
    "        for filename in self.test_cases.keys():\n",
    "            file_path = os.path.join(participant_dir, f\"{filename}.py\")\n",
    "            self.test_file(file_path)\n",
    "        print(\"\\nüìä SUMMARY RESULTS\")\n",
    "        print(self.results_df)\n",
    "        return self.results_df\n",
    "\n",
    "# Example main\n",
    "def main():\n",
    "    TEST_CASES = {\n",
    "        \"task1\": [\n",
    "            # (input, [all possible valid answers])\n",
    "            (([2, 7, 11, 15], 9), [[0, 1],[1, 0]]),  # Only 2+7=9\n",
    "            (([3, 2, 4], 6), [[1, 2]]),  # Only 2+4=6  \n",
    "            (([3, 3], 6), [[0, 1]]),  # Only 3+3=6\n",
    "            (([1, 2, 3, 4], 5), [[0, 3], [1, 2]]),  # 1+4=5 OR 2+3=5\n",
    "            (([1, 2, 3, 4], 10), [[]]),  # No solution\n",
    "            (([], 5), [[]]),  # Empty list\n",
    "        ],\n",
    "        \"task1ai\": [\n",
    "            (([5, 3], 2), [[0, 1], [1, 0]]),  # 5-3=2 OR 3-5=-2? Wait, let's check the logic\n",
    "            (([10, 8, 2], 2), [[0, 1], [1, 0]]),  # 10-8=2 OR 8-10=-2?\n",
    "            (([7, 1, 5, 3], 4), [[0, 3], [2, 1]]),  # 7-3=4 OR 5-1=4\n",
    "            (([15, 10, 5], 5), [[1, 2], [0, 1]]),  # 10-5=5 OR 15-10=5\n",
    "            (([8, 4], 4), [[0, 1]]),  # 8-4=4\n",
    "            (([1, 2, 3], 5), [[]]),  # No pairs with difference of 5\n",
    "            (([], 5), [[]]),  # Empty list\n",
    "        ],\n",
    "    }\n",
    "    tester = Task1Tester(TEST_CASES)\n",
    "    results_df = tester.run_all()\n",
    "    results_df.to_csv(\"task1_results.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595329c6-f53a-4c1f-a38f-fedecbfbec80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b06aae3d-d9c2-471c-a412-f3ba16dddb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurbe\\AppData\\Local\\Temp\\ipykernel_27044\\3015606086.py:103: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.results_df = pd.concat([self.results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tested .\\task2.py ‚Üí Accuracy 100.0%, Time 0.0002s, Memory 0.16KB\n",
      "‚úÖ Tested .\\task2ai.py ‚Üí Accuracy 100.0%, Time 0.0002s, Memory 0.16KB\n",
      "\n",
      "üìä SUMMARY RESULTS\n",
      "         File Function  Accuracy    Time_s  Memory_KB Complexity  \\\n",
      "0    task2.py    task2       1.0  0.000246   0.164062          4   \n",
      "1  task2ai.py  task2ai       1.0  0.000229   0.164062          4   \n",
      "\n",
      "   Maintainability  Pylint_Score  \n",
      "0          67.3218          10.0  \n",
      "1          67.3218          10.0  \n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import tracemalloc\n",
    "import pandas as pd\n",
    "from radon.complexity import cc_visit\n",
    "from radon.metrics import mi_visit\n",
    "\n",
    "class Task2Tester:\n",
    "    def __init__(self, test_cases):\n",
    "        self.test_cases = test_cases\n",
    "        self.results_df = pd.DataFrame(columns=[\n",
    "            \"File\", \"Function\", \"Accuracy\", \"Time_s\", \"Memory_KB\", \"Complexity\", \"Maintainability\", \"Pylint_Score\"\n",
    "        ])\n",
    "    \n",
    "    def load_function(self, file_path):\n",
    "        \"\"\"Load the main function from a Python file (auto-detect function name).\"\"\"\n",
    "        spec = importlib.util.spec_from_file_location(\"module\", file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        # Find candidate functions\n",
    "        candidates = [attr for attr in dir(module) if callable(getattr(module, attr)) and not attr.startswith(\"_\")]\n",
    "        for func_name in candidates:\n",
    "            if any(keyword in func_name.lower() for keyword in ['task','func','main','solution']):\n",
    "                return getattr(module, func_name), func_name\n",
    "        if candidates:\n",
    "            return getattr(module, candidates[0]), candidates[0]\n",
    "        raise AttributeError(f\"No callable function found in {file_path}\")\n",
    "    \n",
    "    def run_test(self, func, test_cases, repeat: int = 3):\n",
    "        \"\"\"Run all test cases, measure accuracy, peak memory, and time.\"\"\"\n",
    "        correct = 0\n",
    "        peak_memory_kb = 0\n",
    "        start_total = time.perf_counter()\n",
    "        \n",
    "        for idx, test_case in enumerate(test_cases):\n",
    "            try:\n",
    "                input_arr, expected = test_case\n",
    "                \n",
    "                # Repeat function execution for smoother timing\n",
    "                mem_peak_case = 0\n",
    "                result = None\n",
    "                for _ in range(repeat):\n",
    "                    arr_copy = input_arr.copy()  # Avoid modifying original test data\n",
    "                    tracemalloc.start()\n",
    "                    start = time.perf_counter()\n",
    "                    result = func(arr_copy)\n",
    "                    end = time.perf_counter()\n",
    "                    current, peak = tracemalloc.get_traced_memory()\n",
    "                    tracemalloc.stop()\n",
    "                    mem_peak_case = max(mem_peak_case, peak / 1024.0)\n",
    "                \n",
    "                peak_memory_kb = max(peak_memory_kb, mem_peak_case)\n",
    "                \n",
    "                if self.compare_results(result, expected):\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    print(f\"‚ùå Input {input_arr} ‚Üí Expected {expected}, Got {result}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error with input {input_arr}: {e}\")\n",
    "        \n",
    "        end_total = time.perf_counter()\n",
    "        execution_time = end_total - start_total\n",
    "        accuracy = correct / len(test_cases)\n",
    "        return accuracy, execution_time, peak_memory_kb\n",
    "    \n",
    "    def compare_results(self, result, expected):\n",
    "        \"\"\"Compare sorting results - exact list comparison.\"\"\"\n",
    "        if result == expected:\n",
    "            return True\n",
    "        # For sorting, we need exact match including order\n",
    "        return False\n",
    "    \n",
    "    def analyze_code_quality(self, file_path):\n",
    "        \"\"\"Complexity, maintainability (radon), pylint.\"\"\"\n",
    "        try:\n",
    "            with open(file_path,'r',encoding='utf-8') as f:\n",
    "                code = f.read()\n",
    "            complexity = sum(block.complexity for block in cc_visit(code))\n",
    "            maintainability = mi_visit(code, True)\n",
    "            pylint_score = self.get_pylint_score(file_path)\n",
    "            return complexity, maintainability, pylint_score\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Code analysis failed for {file_path}: {e}\")\n",
    "            return 0,0,0\n",
    "    \n",
    "    def get_pylint_score(self, file_path):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"pylint\", file_path, \"--score=y\", \"--disable=R,C,W\"],\n",
    "                capture_output=True, text=True, timeout=30\n",
    "            )\n",
    "            for line in result.stdout.splitlines():\n",
    "                if \"rated at\" in line:\n",
    "                    return float(line.split(\"rated at\")[1].split(\"/\")[0].strip())\n",
    "        except Exception:\n",
    "            pass\n",
    "        return 0.0\n",
    "    \n",
    "    def record_results(self, file_path, function_name, accuracy, time_s, memory_kb, complexity, maintainability, pylint_score):\n",
    "        self.results_df = pd.concat([self.results_df, pd.DataFrame([{\n",
    "            \"File\": os.path.basename(file_path),\n",
    "            \"Function\": function_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Time_s\": time_s,\n",
    "            \"Memory_KB\": memory_kb,\n",
    "            \"Complexity\": complexity,\n",
    "            \"Maintainability\": maintainability,\n",
    "            \"Pylint_Score\": pylint_score\n",
    "        }])], ignore_index=True)\n",
    "    \n",
    "    def test_file(self, file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ùå {file_path} not found\")\n",
    "            return\n",
    "        func, func_name = self.load_function(file_path)\n",
    "        test_cases = self.test_cases.get(os.path.basename(file_path).replace(\".py\",\"\"), [])\n",
    "        accuracy, time_s, memory_kb = self.run_test(func, test_cases)\n",
    "        complexity, maintainability, pylint_score = self.analyze_code_quality(file_path)\n",
    "        self.record_results(file_path, func_name, accuracy, time_s, memory_kb, complexity, maintainability, pylint_score)\n",
    "        print(f\"‚úÖ Tested {file_path} ‚Üí Accuracy {accuracy:.1%}, Time {time_s:.4f}s, Memory {memory_kb:.2f}KB\")\n",
    "    \n",
    "    def run_all(self, participant_dir=\".\"):\n",
    "        for filename in self.test_cases.keys():\n",
    "            file_path = os.path.join(participant_dir, f\"{filename}.py\")\n",
    "            self.test_file(file_path)\n",
    "        print(\"\\nüìä SUMMARY RESULTS\")\n",
    "        print(self.results_df)\n",
    "        return self.results_df\n",
    "\n",
    "# Example main\n",
    "def main():\n",
    "    TEST_CASES = {\n",
    "        \"task2\": [\n",
    "            ([10, 7, 8, 9, 1, 5], [1, 5, 7, 8, 9, 10]),\n",
    "            ([64, 34, 25, 12, 22, 11, 90], [11, 12, 22, 25, 34, 64, 90]),\n",
    "            ([5, 2, 8, 1, 9], [1, 2, 5, 8, 9]),\n",
    "            ([1], [1]),\n",
    "            ([], []),\n",
    "            ([3, 3, 3, 3], [3, 3, 3, 3]),\n",
    "            ([2, 1], [1, 2]),\n",
    "            ([5, 4, 3, 2, 1], [1, 2, 3, 4, 5]),\n",
    "        ],\n",
    "        \"task2ai\": [\n",
    "            ([10, 7, 8, 9, 1, 5], [10, 9, 8, 7, 5, 1]),\n",
    "            ([64, 34, 25, 12, 22, 11, 90], [90, 64, 34, 25, 22, 12, 11]),\n",
    "            ([5, 2, 8, 1, 9], [9, 8, 5, 2, 1]),\n",
    "            ([1], [1]),\n",
    "            ([], []),\n",
    "            ([3, 3, 3, 3], [3, 3, 3, 3]),\n",
    "            ([2, 1], [2, 1]),\n",
    "            ([1, 2, 3, 4, 5], [5, 4, 3, 2, 1]),\n",
    "        ],\n",
    "    }\n",
    "    tester = Task2Tester(TEST_CASES)\n",
    "    results_df = tester.run_all()\n",
    "    results_df.to_csv(\"task2_results.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468cb87a-ae76-46e3-8eeb-1de038970d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac40707f-50be-4d12-96df-ed915d26be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurbe\\AppData\\Local\\Temp\\ipykernel_27044\\453332541.py:140: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.results_df = pd.concat([self.results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All results stored in DataFrame 'results_df' for export.\n",
      "      Task  Accuracy    Time_s    Memory_KB Complexity  Maintainability  \\\n",
      "0    task3       1.0  0.044962  5343.778320          1         75.42418   \n",
      "1  task3ai       1.0  0.040859  5343.432617          1         75.42418   \n",
      "\n",
      "   Pylint_Score  \n",
      "0          10.0  \n",
      "1          10.0  \n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import tracemalloc\n",
    "from typing import List, Any, Tuple\n",
    "\n",
    "# Added for better complexity/maintainability\n",
    "from radon.complexity import cc_visit\n",
    "from radon.metrics import mi_visit\n",
    "\n",
    "class Task3Tester:\n",
    "    def __init__(self):\n",
    "        self.csv_path = \"Salary_Dataset.csv\"\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            raise FileNotFoundError(f\"Dataset file {self.csv_path} not found\")\n",
    "        \n",
    "        self.task3_expected = self.calculate_task3_expected()\n",
    "        self.task3ai_expected = self.calculate_task3ai_expected()\n",
    "        # Prepare DataFrame to store results\n",
    "        self.results_df = pd.DataFrame(columns=[\n",
    "            \"Task\", \"Accuracy\", \"Time_s\", \"Memory_KB\", \"Complexity\", \n",
    "            \"Maintainability\", \"Pylint_Score\"\n",
    "        ])\n",
    "    \n",
    "    def calculate_task3_expected(self):\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        android_avg = round(df[df['Job Title'] == 'Android Developer']['Salary'].mean(), 2)\n",
    "        top_companies = df.groupby('Company Name')['Salary'].mean().sort_values(ascending=False).head(5)\n",
    "        return android_avg, top_companies\n",
    "    \n",
    "    def calculate_task3ai_expected(self):\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        company_name = df.loc[df['Salary'].idxmax()]['Company Name']\n",
    "        top_cities = df[df['Employment Status'].str.contains('Full Time', na=False)]['Location'].value_counts().head(10)\n",
    "        return company_name, top_cities\n",
    "    \n",
    "    def load_function(self, file_path: str, function_name: str) -> Any:\n",
    "        spec = importlib.util.spec_from_file_location(\"module\", file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return getattr(module, function_name)\n",
    "    \n",
    "    def measure_peak_memory(self, func, *args, **kwargs) -> Tuple[float, Any]:\n",
    "        \"\"\"Measure peak memory usage across the function call in KB.\"\"\"\n",
    "        tracemalloc.start()\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        execution_time = time.time() - start_time\n",
    "        peak_kb = peak / 1024\n",
    "        return peak_kb, execution_time, result\n",
    "    \n",
    "    def compare_values(self, result, expected) -> bool:\n",
    "        try:\n",
    "            return abs(result - expected) < 0.01 if isinstance(result, (int, float)) else result == expected\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def compare_lists_to_series(self, result_list, expected_series) -> bool:\n",
    "        try:\n",
    "            expected_list = expected_series.index.tolist()\n",
    "            if set(result_list) != set(expected_list) or len(result_list) != len(expected_list):\n",
    "                return False\n",
    "            # Allow ordering differences for ties\n",
    "            result_salaries = [expected_series[company] for company in result_list]\n",
    "            expected_salaries = expected_series.values.tolist()\n",
    "            return all(result_salaries[i] >= result_salaries[i+1] for i in range(len(result_salaries)-1))\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def run_tests(self, func, expected_results, task_name: str) -> Tuple[float, float, float]:\n",
    "        correct = 0\n",
    "        total_tests = 2\n",
    "        memory_kb, exec_time, result = self.measure_peak_memory(func, self.csv_path)\n",
    "        \n",
    "        try:\n",
    "            if task_name == \"task3\":\n",
    "                android_avg, top_companies = result\n",
    "                expected_avg, expected_companies = expected_results\n",
    "                if self.compare_values(android_avg, expected_avg):\n",
    "                    correct += 1\n",
    "                if (isinstance(top_companies, list) and self.compare_lists_to_series(top_companies, expected_companies)) or \\\n",
    "                   (isinstance(top_companies, pd.Series) and top_companies.equals(expected_companies)):\n",
    "                    correct += 1\n",
    "            \n",
    "            elif task_name == \"task3ai\":\n",
    "                company_name, top_cities = result\n",
    "                expected_company, expected_cities = expected_results\n",
    "                if isinstance(company_name, pd.Series):\n",
    "                    company_name = company_name.iloc[0] if len(company_name) > 0 else \"\"\n",
    "                if company_name == expected_company:\n",
    "                    correct += 1\n",
    "                if (isinstance(top_cities, list) and self.compare_lists_to_series(top_cities, expected_cities)) or \\\n",
    "                   (isinstance(top_cities, pd.Series) and top_cities.equals(expected_cities)):\n",
    "                    correct += 1\n",
    "        except Exception:\n",
    "            correct = 0\n",
    "        \n",
    "        accuracy = correct / total_tests\n",
    "        return accuracy, exec_time, memory_kb\n",
    "    \n",
    "    def analyze_code_quality(self, file_path: str) -> Tuple[float, float, float]:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                code = f.read()\n",
    "            # Cyclomatic complexity\n",
    "            functions_cc = cc_visit(code)\n",
    "            complexity = max([func.complexity for func in functions_cc], default=0)\n",
    "            # Maintainability index\n",
    "            maintainability = mi_visit(code, True)\n",
    "            # Pylint score\n",
    "            pylint_score = self.get_pylint_score(file_path)\n",
    "            return complexity, maintainability, pylint_score\n",
    "        except Exception:\n",
    "            return 0, 0, 0\n",
    "    \n",
    "    def get_pylint_score(self, file_path: str) -> float:\n",
    "        try:\n",
    "            result = subprocess.run(['pylint', file_path, '--score=y', '--disable=R,C,W'],\n",
    "                                    capture_output=True, text=True, timeout=30)\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'rated at' in line:\n",
    "                    return float(line.split('rated at')[1].split('/')[0].strip())\n",
    "        except:\n",
    "            return 0.0\n",
    "        return 0.0\n",
    "    \n",
    "    def test_task(self, file_path: str, func_name: str, expected_results, task_name: str):\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ùå {file_path} not found\")\n",
    "            return\n",
    "        func = self.load_function(file_path, func_name)\n",
    "        accuracy, exec_time, memory_kb = self.run_tests(func, expected_results, task_name)\n",
    "        complexity, maintainability, pylint_score = self.analyze_code_quality(file_path)\n",
    "        \n",
    "        # Store results\n",
    "        self.results_df = pd.concat([self.results_df, pd.DataFrame([{\n",
    "            \"Task\": task_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Time_s\": exec_time,\n",
    "            \"Memory_KB\": memory_kb,\n",
    "            \"Complexity\": complexity,\n",
    "            \"Maintainability\": maintainability,\n",
    "            \"Pylint_Score\": pylint_score\n",
    "        }])], ignore_index=True)\n",
    "    \n",
    "    def run_all_tests(self):\n",
    "        self.test_task(\"task3.py\", \"task3\", self.task3_expected, \"task3\")\n",
    "        self.test_task(\"task3ai.py\", \"task3ai\", self.task3ai_expected, \"task3ai\")\n",
    "        print(\"\\n‚úÖ All results stored in DataFrame 'results_df' for export.\")\n",
    "        return self.results_df\n",
    "\n",
    "def main():\n",
    "    tester = Task3Tester()\n",
    "    results_df = tester.run_all_tests()\n",
    "    print(results_df)\n",
    "    # Example export\n",
    "    results_df.to_csv(\"task3_results.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb7cd8d-d659-45e1-8d9d-5b2326a4f807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee34f09-6a28-4adc-870d-77de822ca9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurbe\\AppData\\Local\\Temp\\ipykernel_27044\\492486215.py:111: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.results_df = pd.concat([self.results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä TASK4 RESULTS:\n",
      "    Task  Accuracy    Time_s  Memory_KB Complexity  Maintainability  \\\n",
      "0  task4       1.0  0.000057    0.28125          4        63.421356   \n",
      "\n",
      "   Pylint_Score  \n",
      "0          10.0  \n",
      "\n",
      "üìä TASK4AI RESULTS:\n",
      "      Task  Accuracy    Time_s  Memory_KB Complexity  Maintainability  \\\n",
      "1  task4ai       1.0  0.000076   0.296875          3        65.665012   \n",
      "\n",
      "   Pylint_Score  \n",
      "1          10.0  \n",
      "\n",
      "============================================================\n",
      "ROUND-TRIP TESTING\n",
      "============================================================\n",
      "‚úÖ 1 -> I -> 1\n",
      "‚úÖ 4 -> IV -> 4\n",
      "‚úÖ 9 -> IX -> 9\n",
      "‚úÖ 49 -> XLIX -> 49\n",
      "‚úÖ 99 -> XCIX -> 99\n",
      "‚úÖ 499 -> CDXCIX -> 499\n",
      "‚úÖ 999 -> CMXCIX -> 999\n",
      "‚úÖ 1499 -> MCDXCIX -> 1499\n",
      "‚úÖ 1999 -> MCMXCIX -> 1999\n",
      "‚úÖ 2499 -> MMCDXCIX -> 2499\n",
      "‚úÖ 2999 -> MMCMXCIX -> 2999\n",
      "‚úÖ 3499 -> MMMCDXCIX -> 3499\n",
      "‚úÖ 3999 -> MMMCMXCIX -> 3999\n",
      "\n",
      "üìä ROUND-TRIP ACCURACY: 100.0%\n",
      "\n",
      "‚úÖ ALL TEST RESULTS:\n",
      "         Task  Accuracy    Time_s  Memory_KB Complexity  Maintainability  \\\n",
      "0       task4       1.0  0.000057   0.281250          4        63.421356   \n",
      "1     task4ai       1.0  0.000076   0.296875          3        65.665012   \n",
      "2  round_trip       1.0  0.000000   0.000000          0         0.000000   \n",
      "\n",
      "   Pylint_Score  \n",
      "0          10.0  \n",
      "1          10.0  \n",
      "2           0.0  \n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import time\n",
    "import os\n",
    "import tracemalloc\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from radon.complexity import cc_visit\n",
    "from radon.metrics import mi_visit\n",
    "\n",
    "class Task4Tester:\n",
    "    def __init__(self):\n",
    "        # Test cases for task4 (Roman to Integer)\n",
    "        self.task4_test_cases = [\n",
    "            (\"III\", 3), (\"IV\", 4), (\"IX\", 9), (\"LVIII\", 58), (\"MCMXC\", 1990),\n",
    "            (\"MMXXIV\", 2024), (\"XIV\", 14), (\"XCIX\", 99), (\"CDXLIV\", 444),\n",
    "            (\"MCMXCIX\", 1999), (\"I\", 1), (\"V\", 5), (\"X\", 10), (\"L\", 50),\n",
    "            (\"C\", 100), (\"D\", 500), (\"M\", 1000),\n",
    "        ]\n",
    "        # Test cases for task4ai (Integer to Roman)\n",
    "        self.task4ai_test_cases = [\n",
    "            (3, \"III\"), (4, \"IV\"), (9, \"IX\"), (58, \"LVIII\"), (1990, \"MCMXC\"),\n",
    "            (2024, \"MMXXIV\"), (14, \"XIV\"), (99, \"XCIX\"), (444, \"CDXLIV\"),\n",
    "            (1999, \"MCMXCIX\"), (1, \"I\"), (5, \"V\"), (10, \"X\"), (50, \"L\"),\n",
    "            (100, \"C\"), (500, \"D\"), (1000, \"M\"), (3999, \"MMMCMXCIX\")\n",
    "        ]\n",
    "        # Results DataFrame\n",
    "        self.results_df = pd.DataFrame(columns=[\n",
    "            \"Task\", \"Accuracy\", \"Time_s\", \"Memory_KB\", \"Complexity\", \"Maintainability\", \"Pylint_Score\"\n",
    "        ])\n",
    "    \n",
    "    def load_function(self, file_path: str, function_name: str):\n",
    "        \"\"\"Load a specific function from a Python file.\"\"\"\n",
    "        spec = importlib.util.spec_from_file_location(\"module\", file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return getattr(module, function_name)\n",
    "    \n",
    "    def measure_memory_and_time(self, func, *args, repeat: int = 1) -> Tuple[float, float, any]:\n",
    "        \"\"\"Measure execution time and peak memory (KB) across repeat runs.\"\"\"\n",
    "        peak_memory_kb = 0\n",
    "        start_total = time.time()\n",
    "        result = None\n",
    "        for i in range(repeat):\n",
    "            tracemalloc.start()\n",
    "            start = time.time()\n",
    "            result = func(*args)\n",
    "            end = time.time()\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            peak_memory_kb = max(peak_memory_kb, peak / 1024)\n",
    "        end_total = time.time()\n",
    "        total_time = end_total - start_total\n",
    "        return total_time, peak_memory_kb, result\n",
    "    \n",
    "    def run_tests(self, func, test_cases: List[Tuple], repeat: int = 1) -> Tuple[float, float, float]:\n",
    "        \"\"\"Run all test cases and return accuracy, time, and memory usage.\"\"\"\n",
    "        correct = 0\n",
    "        memory_kb = 0\n",
    "        start_time = time.time()\n",
    "        for idx, (inp, expected) in enumerate(test_cases):\n",
    "            try:\n",
    "                # Measure memory only for the first case\n",
    "                if idx == 0:\n",
    "                    exec_time, mem_kb, result = self.measure_memory_and_time(func, inp, repeat=repeat)\n",
    "                    memory_kb = mem_kb\n",
    "                else:\n",
    "                    result = func(inp)\n",
    "                if result == expected:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    print(f\"‚ùå Input: {inp}, Expected: {expected}, Got: {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error with input {inp}: {e}\")\n",
    "        accuracy = correct / len(test_cases)\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        return accuracy, total_time, memory_kb\n",
    "    \n",
    "    def analyze_code_quality(self, file_path: str) -> Tuple[float, float, float]:\n",
    "        \"\"\"Analyze code complexity, maintainability, and pylint score.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                code = f.read()\n",
    "            # Cyclomatic complexity using radon\n",
    "            complexity = sum([block.complexity for block in cc_visit(code)])\n",
    "            # Maintainability Index\n",
    "            maintainability = mi_visit(code, True)\n",
    "            # Pylint score\n",
    "            pylint_score = self.get_pylint_score(file_path)\n",
    "            return complexity, maintainability, pylint_score\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Code analysis failed: {e}\")\n",
    "            return 0, 0, 0\n",
    "    \n",
    "    def get_pylint_score(self, file_path: str) -> float:\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"pylint\", file_path, \"--score=y\", \"--disable=R,C,W\"],\n",
    "                capture_output=True, text=True, timeout=30\n",
    "            )\n",
    "            for line in result.stdout.splitlines():\n",
    "                if \"rated at\" in line:\n",
    "                    score = float(line.split(\"rated at\")[1].split(\"/\")[0].strip())\n",
    "                    return score\n",
    "        except Exception:\n",
    "            pass\n",
    "        return 0.0\n",
    "    \n",
    "    def record_results(self, task_name, accuracy, time_s, memory_kb, complexity, maintainability, pylint_score):\n",
    "        self.results_df = pd.concat([self.results_df, pd.DataFrame([{\n",
    "            \"Task\": task_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Time_s\": time_s,\n",
    "            \"Memory_KB\": memory_kb,\n",
    "            \"Complexity\": complexity,\n",
    "            \"Maintainability\": maintainability,\n",
    "            \"Pylint_Score\": pylint_score\n",
    "        }])], ignore_index=True)\n",
    "    \n",
    "    def test_task(self, file_path: str, function_name: str, test_cases: List[Tuple], task_name: str):\n",
    "        \"\"\"General testing routine for Roman/Integer conversion tasks.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ùå {file_path} not found\")\n",
    "            return\n",
    "        func = self.load_function(file_path, function_name)\n",
    "        accuracy, time_s, memory_kb = self.run_tests(func, test_cases, repeat=3)\n",
    "        complexity, maintainability, pylint_score = self.analyze_code_quality(file_path)\n",
    "        self.record_results(task_name, accuracy, time_s, memory_kb, complexity, maintainability, pylint_score)\n",
    "        print(f\"\\nüìä {task_name.upper()} RESULTS:\")\n",
    "        print(self.results_df[self.results_df[\"Task\"]==task_name])\n",
    "    \n",
    "    def test_round_trip(self):\n",
    "        \"\"\"Test round-trip Roman <-> Integer conversions.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ROUND-TRIP TESTING\")\n",
    "        print(\"=\"*60)\n",
    "        if not os.path.exists(\"task4.py\") or not os.path.exists(\"task4ai.py\"):\n",
    "            print(\"‚ùå Both task4.py and task4ai.py required\")\n",
    "            return\n",
    "        roman_to_int = self.load_function(\"task4.py\", \"task4\")\n",
    "        int_to_roman = self.load_function(\"task4ai.py\", \"task4ai\")\n",
    "        passed = 0\n",
    "        round_trip_cases = [1, 4, 9, 49, 99, 499, 999, 1499, 1999, 2499, 2999, 3499, 3999]\n",
    "        for num in round_trip_cases:\n",
    "            roman = int_to_roman(num)\n",
    "            back_to_num = roman_to_int(roman)\n",
    "            if num == back_to_num:\n",
    "                passed += 1\n",
    "                print(f\"‚úÖ {num} -> {roman} -> {back_to_num}\")\n",
    "            else:\n",
    "                print(f\"‚ùå {num} -> {roman} -> {back_to_num} (expected {num})\")\n",
    "        accuracy = passed / len(round_trip_cases)\n",
    "        self.record_results(\"round_trip\", accuracy, 0, 0, 0, 0, 0)\n",
    "        print(f\"\\nüìä ROUND-TRIP ACCURACY: {accuracy:.1%}\")\n",
    "    \n",
    "    def run_all(self):\n",
    "        self.test_task(\"task4.py\", \"task4\", self.task4_test_cases, \"task4\")\n",
    "        self.test_task(\"task4ai.py\", \"task4ai\", self.task4ai_test_cases, \"task4ai\")\n",
    "        self.test_round_trip()\n",
    "        print(\"\\n‚úÖ ALL TEST RESULTS:\")\n",
    "        print(self.results_df)\n",
    "\n",
    "def main():\n",
    "    tester = Task4Tester()\n",
    "    tester.run_all()\n",
    "    # Export results if desired\n",
    "    tester.results_df.to_csv(\"task4_results.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c673f37-3b27-48b3-96b6-b7e1a42b5f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "187afe8d-40b3-422d-85de-b2d7d8a15b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task0=pd.read_csv('task0_results.csv')\n",
    "df_task0.pop('File')\n",
    "df_task1=pd.read_csv('task1_results.csv')\n",
    "df_task1.pop('File')\n",
    "df_task2=pd.read_csv('task2_results.csv')\n",
    "df_task2.pop('File')\n",
    "df_task3=pd.read_csv('task3_results.csv')\n",
    "df_task3=df_task3.rename(columns={\"Task\": \"Function\"})\n",
    "df_task4=pd.read_csv('task4_results.csv')\n",
    "df_task4=df_task4.rename(columns={\"Task\": \"Function\"})\n",
    "df_task4 = df_task4[df_task4['Function']!= 'round_trip']\n",
    "dfs = [df_task0, df_task1, df_task2, df_task3, df_task4]\n",
    "df_all = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4022df78-3380-46a3-802b-ff83304dd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"df_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907830e-46f5-4196-81bc-670eedb40d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
